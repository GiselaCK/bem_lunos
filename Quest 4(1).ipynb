{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a24d05-a31c-4595-974d-f65ec2d2b519",
   "metadata": {},
   "source": [
    "---\n",
    "## **INTRODUÇÃO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc15cc-03e2-47db-ac73-d86a6898cab7",
   "metadata": {},
   "source": [
    "---\n",
    "## **DESENVOLVIMENTO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a763f-3b88-4cd8-a80e-b0c12605922c",
   "metadata": {},
   "source": [
    "---\n",
    "### **DOWNLOAD DO DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58f7e9-e896-4f6e-b32b-50a2c95cbc06",
   "metadata": {},
   "source": [
    "Para iniciar nossa análise, utilizaremos o dataset \"Student Performance Factors\" disponível no Kaggle. Realizamos o download do conjunto de dados através do link  https://teams.microsoft.com/l/message/19:e98de34e8e034b56b694d6cdac057368@thread.v2/1760965275974?context=%7B%22contextType%22%3A%22chat%22%7D e extraímos o arquivo compactado em formato ZIP. O arquivo resultante, em formato CSV, foi armazenado no diretório da Quest 4 do projeto no Jupyter Notebook. Agora, procederemos com a leitura e carregamento dos dados para dar início ao processo de exploração e modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9f67024-e231-4a5b-a8f9-bac780ae430f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Hours_Studied  Attendance Parental_Involvement Access_to_Resources  \\\n",
      "0                23          84                  Low                High   \n",
      "1                19          64                  Low              Medium   \n",
      "2                24          98               Medium              Medium   \n",
      "3                29          89                  Low              Medium   \n",
      "4                19          92               Medium              Medium   \n",
      "...             ...         ...                  ...                 ...   \n",
      "6602             25          69                 High              Medium   \n",
      "6603             23          76                 High              Medium   \n",
      "6604             20          90               Medium                 Low   \n",
      "6605             10          86                 High                High   \n",
      "6606             15          67               Medium                 Low   \n",
      "\n",
      "     Extracurricular_Activities  Sleep_Hours  Previous_Scores  \\\n",
      "0                            No            7               73   \n",
      "1                            No            8               59   \n",
      "2                           Yes            7               91   \n",
      "3                           Yes            8               98   \n",
      "4                           Yes            6               65   \n",
      "...                         ...          ...              ...   \n",
      "6602                         No            7               76   \n",
      "6603                         No            8               81   \n",
      "6604                        Yes            6               65   \n",
      "6605                        Yes            6               91   \n",
      "6606                        Yes            9               94   \n",
      "\n",
      "     Motivation_Level Internet_Access  Tutoring_Sessions Family_Income  \\\n",
      "0                 Low             Yes                  0           Low   \n",
      "1                 Low             Yes                  2        Medium   \n",
      "2              Medium             Yes                  2        Medium   \n",
      "3              Medium             Yes                  1        Medium   \n",
      "4              Medium             Yes                  3        Medium   \n",
      "...               ...             ...                ...           ...   \n",
      "6602           Medium             Yes                  1          High   \n",
      "6603           Medium             Yes                  3           Low   \n",
      "6604              Low             Yes                  3           Low   \n",
      "6605             High             Yes                  2           Low   \n",
      "6606           Medium             Yes                  0        Medium   \n",
      "\n",
      "     Teacher_Quality School_Type Peer_Influence  Physical_Activity  \\\n",
      "0             Medium      Public       Positive                  3   \n",
      "1             Medium      Public       Negative                  4   \n",
      "2             Medium      Public        Neutral                  4   \n",
      "3             Medium      Public       Negative                  4   \n",
      "4               High      Public        Neutral                  4   \n",
      "...              ...         ...            ...                ...   \n",
      "6602          Medium      Public       Positive                  2   \n",
      "6603            High      Public       Positive                  2   \n",
      "6604          Medium      Public       Negative                  2   \n",
      "6605          Medium     Private       Positive                  3   \n",
      "6606          Medium      Public       Positive                  4   \n",
      "\n",
      "     Learning_Disabilities Parental_Education_Level Distance_from_Home  \\\n",
      "0                       No              High School               Near   \n",
      "1                       No                  College           Moderate   \n",
      "2                       No             Postgraduate               Near   \n",
      "3                       No              High School           Moderate   \n",
      "4                       No                  College               Near   \n",
      "...                    ...                      ...                ...   \n",
      "6602                    No              High School               Near   \n",
      "6603                    No              High School               Near   \n",
      "6604                    No             Postgraduate               Near   \n",
      "6605                    No              High School                Far   \n",
      "6606                    No             Postgraduate               Near   \n",
      "\n",
      "      Gender  Exam_Score  \n",
      "0       Male          67  \n",
      "1     Female          61  \n",
      "2       Male          74  \n",
      "3       Male          71  \n",
      "4     Female          70  \n",
      "...      ...         ...  \n",
      "6602  Female          68  \n",
      "6603  Female          69  \n",
      "6604  Female          68  \n",
      "6605  Female          68  \n",
      "6606    Male          64  \n",
      "\n",
      "[6607 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('StudentPerformanceFactors.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce161e94-ffd8-441a-9c55-27acd8e03b4d",
   "metadata": {},
   "source": [
    "---\n",
    "### **TRATAMENTO DE DADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9432913-4f8c-4eb4-8d28-2f771e340aaa",
   "metadata": {},
   "source": [
    "No tratamento de dados, será realizado três passos:\n",
    "1. Retirar linhas que não tenham algum valor, para evitar erros no treinamento;\n",
    "2. Converter os dados categóricos (ordinais e binários) para numéricos, já que nosso target é numérico e usaremos regressão para o treinamento;\n",
    "3. Dividir os dados entre treino e teste para aplicar no modelo.\n",
    "   \n",
    "Vamos começar com o primeiro tópico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a55aea9b-d6ff-4a64-974f-4e0ac5edfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns #para carregar datasets prontos;\n",
    "import pandas #para manipulação de dados;\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler #para conversão de dados categóricos;\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop de colunas com QUALQUER linha NaN\n",
    "df = df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab735342-b6fd-4472-bb8b-c00ed728d319",
   "metadata": {},
   "source": [
    "#### **CONVERSÃO DE DADOS CATEGÓRICOS PARA NUMÉRICOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb16a6-1f5f-4098-b9f7-c787d894a10a",
   "metadata": {},
   "source": [
    "Abaixo segue o código que irá converter dados categóricos ordinais com o OrdinalEncoder, atribuindo valores numéricos correspondentes à ordem de cada categoria, e dados binários a partir de \"mapping\" cada label com 0 ou 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e1d72b0-ba0c-4a55-a788-c3395bf615c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricos = {\n",
    "    'Parental_Involvement': ['Low', 'Medium', 'High'],\n",
    "    'Access_to_Resources': ['Low', 'Medium', 'High'],\n",
    "    'Motivation_Level': ['Low', 'Medium', 'High'],\n",
    "    'Family_Income': ['Low', 'Medium', 'High'],\n",
    "    'Teacher_Quality': ['Low', 'Medium', 'High'],\n",
    "    'Distance_from_Home': ['Near', 'Moderate', 'Far'],\n",
    "    'Peer_Influence': ['Negative', 'Neutral', 'Positive'],\n",
    "    'Parental_Education_Level': ['High School', 'College', 'Postgraduate'] \n",
    "}\n",
    "\n",
    "binarios = {\n",
    "    'Gender': {'Male': 0, 'Female': 1},\n",
    "    'Learning_Disabilities': {'No': 0, 'Yes': 1},\n",
    "    'Extracurricular_Activities': {'No': 0, 'Yes': 1},\n",
    "    'Internet_Access': {'No': 0, 'Yes': 1},\n",
    "    'School_Type': {'Public': 0, 'Private': 1}\n",
    "}\n",
    "\n",
    "for coluna, ordem in categoricos.items():\n",
    "    encoder = OrdinalEncoder(categories=[ordem])\n",
    "    df[coluna] = encoder.fit_transform(df[[coluna]])\n",
    "\n",
    "for coluna, mapa in binarios.items():\n",
    "    df[coluna] = df[coluna].map(mapa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97053719-6c95-4c4f-96c6-9acd0d3de81e",
   "metadata": {},
   "source": [
    "#### **SPLIT E NORMALIZAÇÃO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b783f2-9815-4d46-a452-d4454f658fd3",
   "metadata": {},
   "source": [
    "Nesta seção, iremos dividir nossa dataset entre treino e teste para poder criar e treinar os modelos. Além disso, vamos definir nossos atributos e o nosso target, que será o exam score. Com esses objetivos, vamos usar funções do `sklearn.model_selection`. Para a normalização, foi escolhido o `StandardScaler`.\n",
    "\n",
    "Note que a normalização com StandardScaler é crucial para KNN, Ridge Regression e Elastic Net devido à sua sensibilidade à escala dos dados. O KNN baseia-se em cálculos de distância onde features em escalas diferentes distorceriam as medidas de similaridade. Já Ridge e Elastic Net aplicam penalidades de regularização que seriam injustamente influenciadas por variáveis com magnitude naturalmente maior, comprometendo a eficácia do modelo. O StandardScaler padroniza os dados removendo a média e escalando para variância unitária, transformando cada feature para ter média zero e desvio padrão igual a 1. Essa transformação garante que todas as features contribuam igualmente para os algoritmos, independentemente de suas escalas originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "271c1a24-a66a-4e0b-8501-b79ec966394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Hours_Studied', 'Attendance', 'Parental_Involvement',\n",
      "       'Access_to_Resources', 'Extracurricular_Activities', 'Sleep_Hours',\n",
      "       'Previous_Scores', 'Motivation_Level', 'Internet_Access',\n",
      "       'Tutoring_Sessions', 'Family_Income', 'Teacher_Quality', 'School_Type',\n",
      "       'Peer_Influence', 'Physical_Activity', 'Learning_Disabilities',\n",
      "       'Parental_Education_Level', 'Distance_from_Home', 'Gender',\n",
      "       'Exam_Score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2386ca9-ef92-4f64-ad31-afc632361213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #Para dividir entre treino e teste do modelo\n",
    "\n",
    "FEATURES = ['Hours_Studied', 'Attendance', 'Parental_Involvement',\n",
    "       'Access_to_Resources', 'Extracurricular_Activities', 'Sleep_Hours',\n",
    "       'Previous_Scores', 'Motivation_Level', 'Internet_Access',\n",
    "       'Tutoring_Sessions', 'Family_Income', 'Teacher_Quality', 'School_Type',\n",
    "       'Peer_Influence', 'Physical_Activity', 'Learning_Disabilities',\n",
    "       'Parental_Education_Level', 'Distance_from_Home', 'Gender']\n",
    "#definindo o alvo que será previsto\n",
    "TARGET = ['Exam_Score']\n",
    "    \n",
    "#definindo na variável X os valores de cada coluna que é parâmetro\n",
    "X = df[FEATURES].values\n",
    "    \n",
    "#definindo na variável y os valores da coluna alvo e transformando em unidimensional com o método \".ravel()\"\n",
    "y = df[TARGET].values.ravel()\n",
    "\n",
    "#dividindo entre teste e treino, com uma porcentagem de 20% para teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "normalizador = StandardScaler()\n",
    "X_treino_norm = normalizador.fit_transform(X_treino)\n",
    "X_teste_norm = normalizador.transform(X_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e099343-2504-48dc-970c-69be4dac0664",
   "metadata": {},
   "source": [
    "---\n",
    "### **BASELINE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a2fc3-f0d5-4d5e-a3db-883251d30c24",
   "metadata": {},
   "source": [
    "Nesta seção, estabeleceremos uma baseline de performance como referência fundamental para avaliar a eficácia dos nossos modelos de regressão. A baseline representa o desempenho mínimo que nossos algoritmos devem superar para demonstrar valor preditivo. Utilizaremos abordagens simples, prevendo constantemente a média dos valores de treino. Esta prática é crucial para validar que a complexidade adicional dos algoritmos de machine learning está de fato gerando ganhos preditivos significativos em relação a soluções triviais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccf67fc0-5397-45f9-ba03-70308cc87c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9513102465608547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calcula a média do target no treino\n",
    "media = np.mean(y_treino)\n",
    "\n",
    "# Cria previsões (sempre a média)\n",
    "y_pred_baseline = np.full_like(y_teste, fill_value=media)\n",
    "\n",
    "# Calcula RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_baseline)\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9e62d-3233-464b-9f15-d0d361cb9848",
   "metadata": {},
   "source": [
    "---\n",
    "### **CRIANDO O MODELO E TREINANDO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be9bf4-3349-4cb0-a1e2-25b2312f2cda",
   "metadata": {},
   "source": [
    "Nesta seção, aplicaremos cinco algoritmos de regressão distintos - K-Nearest Neighbors (KNN), Random Forest, Elastic Net, Gradient Boosting Regressor e Ridge Regression - para treinar modelos preditivos utilizando nosso conjunto de dados. Para cada algoritmo, iniciaremos com uma fundamentação teórica que explica seus princípios de funcionamento e características principais, seguida pela implementação prática utilizando a biblioteca scikit-learn.\n",
    "\n",
    "É importante destacar que, nesta fase inicial de modelagem, utilizaremos os valores padrão de hiperparâmetros definidos pelo scikit-learn para cada algoritmo. Esta abordagem nos permitirá estabelecer uma linha de base de performance antes de proceder com a otimização sistemática de hiperparâmetros, que será realizada em etapas subsequentes do projeto.\n",
    "\n",
    "Para avalisar o desempenho dos algoritmos neste primeiro momento, será usado o Root Mean Squared Error (RMSE) - uma métrica que quantifica a magnitude média dos erros de previsão. É a raiz quadrada da média dos quadrados das diferenças entre valores observados e previstos e está na mesma unidade da variável target. Por utilizar o quadrado dos erros, esta métrica penaliza mais fortemente previsões que estão significativamente distantes dos valores reais, sendo particularmente sensível a outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656795d-36a7-40f3-90e7-5ee4edc0d777",
   "metadata": {},
   "source": [
    "#### **K-nearest neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df28f8-4caf-467c-a465-e9e4b3d9ca69",
   "metadata": {},
   "source": [
    "O algoritmo K-Nearest Neighbors (KNN) é um método de aprendizado baseado em instâncias que realiza previsões calculando a similaridade entre observações no espaço de features. Quando aplicado a problemas de regressão, o KNN identifica os k pontos de dados mais próximos da observação que se deseja prever, utilizando métricas de distância como Euclidiana ou Manhattan, e então calcula a média dos valores target desses vizinhos para gerar a previsão final. Esta abordagem não constrói um modelo explícito durante o treinamento, mas armazena todo o conjunto de dados, realizando os cálculos apenas no momento da inferência, o que o torna computacionalmente intensivo para conjuntos \n",
    "grandes, porém flexível para capturar padrões complexos não-lineares nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c99474ad-f46f-4125-92dc-6112ae64221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.783189573246307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor #Para aplicar o modelo\n",
    "from sklearn.metrics import root_mean_squared_error #Para avaliar seu desempenho\n",
    " \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_treino_scaled = scaler.fit_transform(X_treino)\n",
    "X_teste_scaled = scaler.transform(X_teste)\n",
    "\n",
    "#criando modelo\n",
    "modelo_knn = KNeighborsRegressor()\n",
    "modelo_knn.fit(X_treino_scaled, y_treino)\n",
    "    \n",
    "#treinando o modelo\n",
    "y_pred_knn = modelo_knn.predict(X_teste_scaled)\n",
    "\n",
    "#avaliando o desempenho com RMSE (que será nosso retorno) a partir do target previsto\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_knn)\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c701e84-2ee9-41f6-8c3d-c2e782fad5b3",
   "metadata": {},
   "source": [
    "#### **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be016a-e5b7-4953-a50c-ea5928c9b0cf",
   "metadata": {},
   "source": [
    "Random Forest Regression é um método de ensemble baseado em bagging (Bootstrap Aggregating) que constrói múltiplas árvores de decisão independentes e combina suas previsões através da média. O bagging é uma técnica que reduz a variância do modelo através da criação de múltiplos conjuntos de treinamento via amostragem com reposição (bootstrap) e da agregação de seus resultados. Durante o treinamento, cada árvore é construída usando uma amostra bootstrap do conjunto de dados original e, em cada divisão de nó, apenas um subconjunto aleatório de features é considerado para seleção, introduzindo dupla aleatorização que promove diversidade entre as árvores e reduz significativamente o overfitting, resultando em um modelo robusto que geralmente apresenta excelente performance com pouca necessidade de ajuste fino de hiperparâmetros e natural resistência a outliers e ruídos nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "788aefd8-b74c-4aed-8b8b-c6c65ba00031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4030402960784496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Criando modelo\n",
    "modelo_rf = RandomForestRegressor()\n",
    "    \n",
    "# Treinando o modelo\n",
    "modelo_rf.fit(X_treino, y_treino)\n",
    "    \n",
    "# Fazendo previsões\n",
    "y_pred_rf = modelo_rf.predict(X_teste)\n",
    "    \n",
    "# Avaliando o desempenho com RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_rf)\n",
    "    \n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952431d-bf44-4534-ab00-5e8f13616b09",
   "metadata": {},
   "source": [
    "#### **Gradient Boosting Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1062d3b-69ae-45f3-b7a4-2bd82555a46a",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression é uma técnica de ensemble sequencial que constrói um modelo preditivo forte através da combinação iterativa de múltiplos modelos fracos. O termo ensemble refere-se à abordagem que combina múltiplos modelos base para criar um predictor mais robusto e preciso do que qualquer modelo individual. O algoritmo opera ajustando sequencialmente novos modelos aos resíduos (erros) dos modelos anteriores, utilizando gradiente descendente para minimizar uma função de perda diferenciável, onde cada nova árvore é treinada para prever os gradientes negativos dos erros cometidos pelas árvores existentes, criando assim um processo de melhoria contínua que gradualmente reduz o viés do modelo enquanto mantém a variância controlada através de parâmetros como taxa de aprendizado e profundidade das árvores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0620f98c-0209-4394-8997-a3d2b0b64da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1534226663806626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Criando modelo\n",
    "modelo_gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo_gbr.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred_gbr = modelo_gbr.predict(X_teste)\n",
    "\n",
    "# Avaliando o desempenho com RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_gbr)\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdffa5-3b6d-49eb-9546-13abee2ee125",
   "metadata": {},
   "source": [
    "#### **Elastic Net**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a04dd1-a962-4741-9f58-55f8c13873bb",
   "metadata": {},
   "source": [
    "Elastic Net é um algoritmo de regressão regularizada que combina as penalidades L1 (Lasso) e L2 (Ridge) em uma única função objetivo. A penalidade L1 promove esparsidade nos coeficientes, efetivamente realizando seleção de variáveis ao zerar coeficientes de features menos importantes, enquanto a penalidade L2 contrai uniformemente todos os coeficientes para lidar com multicolinearidade. Durante o treinamento, o algoritmo adiciona um termo de penalização à função de custo tradicional que é uma combinação linear das normas L1 e L2 dos coeficientes, controlada pelo parâmetro alpha para a força total da regularização e l1_ratio para o balanço entre as duas penalidades, resultando em um modelo que pode efetivamente reduzir overfitting enquanto mantém a interpretabilidade através da criação de coeficientes exatamente zero para features irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "891931e6-de12-4b56-b619-44c56265cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9475110833146863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Criando modelo\n",
    "modelo_en = ElasticNet()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo_en.fit(X_treino_scaled, y_treino)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred_en = modelo_en.predict(X_teste_scaled)\n",
    "\n",
    "# Avaliando o desempenho com RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_en)\n",
    "    \n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c97245-4011-45bc-9730-3c9df5aa9487",
   "metadata": {},
   "source": [
    "#### **Ridge Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02803817-ff97-42e1-95a2-955ce857ceb5",
   "metadata": {},
   "source": [
    "Ridge Regression é uma extensão da regressão linear ordinária que adiciona uma penalidade L2 (soma dos quadrados dos coeficientes) à função de custo para lidar com problemas de multicolinearidade e overfitting. A penalidade L2, também conhecida como regularização de Tikhonov, adiciona o quadrado da magnitude dos coeficientes à função de custo, efetivamente contraindo todos os coeficientes proporcionalmente sem eliminá-los completamente. Ao incorporar este termo de regularização controlado pelo parâmetro alpha, o algoritmo estabiliza as estimativas na presença de features altamente correlacionadas e melhora a generalização do modelo, mantendo toda a interpretabilidade da regressão linear tradicional enquanto oferece melhor performance preditiva em situações onde a matriz de design é mal-condicionada ou o número de features é grande relativo ao número de observações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fd3f61f-c6a2-4173-b5f8-b8567fe4ba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.037888341089658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_treino_scaled = scaler.fit_transform(X_treino)\n",
    "X_teste_scaled = scaler.transform(X_teste)\n",
    "\n",
    "# Criando modelo\n",
    "modelo_rr = Ridge()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo_rr.fit(X_treino_scaled, y_treino)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred_rr = modelo_rr.predict(X_teste_scaled)\n",
    "\n",
    "# Avaliando o desempenho com RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_rr)\n",
    "\n",
    "print(RMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab1857",
   "metadata": {},
   "source": [
    "---\n",
    "### **SELEÇÃO DE ATRIBUTOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efd6c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     feature  importance\n",
      "1                 Attendance    0.379373\n",
      "0              Hours_Studied    0.244850\n",
      "6            Previous_Scores    0.086903\n",
      "9          Tutoring_Sessions    0.034246\n",
      "3        Access_to_Resources    0.033835\n",
      "2       Parental_Involvement    0.032872\n",
      "14         Physical_Activity    0.027515\n",
      "5                Sleep_Hours    0.024340\n",
      "10             Family_Income    0.020852\n",
      "16  Parental_Education_Level    0.016734\n"
     ]
    }
   ],
   "source": [
    "importancia = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "importancia.fit(X_treino_norm, y_treino)\n",
    "importancia_features = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': importancia.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(importancia_features.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c351e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_features = importancia_features.head(10)['feature'].tolist()\n",
    "\n",
    "X = df[top_10_features]\n",
    "y = df[TARGET]\n",
    "\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "normalizador = StandardScaler()\n",
    "X_treino_norm = normalizador.fit_transform(X_treino)\n",
    "X_teste_norm = normalizador.transform(X_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024501b0-3f00-45e6-8c90-6cef9ddbfa87",
   "metadata": {},
   "source": [
    "---\n",
    "### **OTIMIZAÇÃO DE HIPERPARÂMETROS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "632582c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "467417ba-141b-43fd-8ea0-68222031bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_knn(trial):\n",
    "    n_vizinhos = trial.suggest_int('n_neighbors', 1, 50)\n",
    "    pesos = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    p = trial.suggest_int('p', 1, 2)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', KNeighborsRegressor(\n",
    "            n_neighbors=n_vizinhos,\n",
    "            weights=pesos,\n",
    "            p=p\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "973cdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_rf(trial):\n",
    "    n_estimadores = trial.suggest_int('n_estimators', 50, 500)\n",
    "    profundidade_maxima = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_amostras_divisao = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_amostras_folha = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_caracteristicas = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators=n_estimadores,\n",
    "            max_depth=profundidade_maxima,\n",
    "            min_samples_split=min_amostras_divisao,\n",
    "            min_samples_leaf=min_amostras_folha,\n",
    "            max_features=max_caracteristicas,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4af2037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_gb(trial):\n",
    "    n_estimadores = trial.suggest_int('n_estimators', 50, 500)\n",
    "    taxa_aprendizado = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    profundidade_maxima = trial.suggest_int('max_depth', 3, 10)\n",
    "    min_amostras_divisao = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_amostras_folha = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    sub_amostra = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', GradientBoostingRegressor(\n",
    "            n_estimators=n_estimadores,\n",
    "            learning_rate=taxa_aprendizado,\n",
    "            max_depth=profundidade_maxima,\n",
    "            min_samples_split=min_amostras_divisao,\n",
    "            min_samples_leaf=min_amostras_folha,\n",
    "            subsample=sub_amostra,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0485ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_elasticnet(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.001, 10.0, log=True)\n",
    "    razao_l1 = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', ElasticNet(\n",
    "            alpha=alpha,\n",
    "            l1_ratio=razao_l1,\n",
    "            random_state=42,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ba3db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_ridge(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.001, 10.0, log=True)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', Ridge(\n",
    "            alpha=alpha,\n",
    "            random_state=42,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f354df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando a otimização do modelo KNN...\")\n",
    "estudo_knn = optuna.create_study(direction='maximize')\n",
    "estudo_knn.optimize(objetivo_knn, n_trials=100)\n",
    "\n",
    "print(\"Iniciando a otimização do modelo Floresta Aleatória...\")\n",
    "estudo_rf = optuna.create_study(direction='maximize')\n",
    "estudo_rf.optimize(objetivo_rf, n_trials=100)\n",
    "\n",
    "print(\"Iniciando a otimização do modelo Gradient Boosting...\")\n",
    "estudo_gb = optuna.create_study(direction='maximize')\n",
    "estudo_gb.optimize(objetivo_gb, n_trials=100)\n",
    "\n",
    "print(\"Iniciando a otimização do modelo Elastic Net...\")\n",
    "estudo_en = optuna.create_study(direction='maximize')\n",
    "estudo_en.optimize(objetivo_elasticnet, n_trials=100)\n",
    "\n",
    "print(\"Iniciando a otimização do modelo Ridge Regression...\")\n",
    "estudo_ridge = optuna.create_study(direction='maximize')\n",
    "estudo_ridge.optimize(objetivo_ridge, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff68c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "melhores_params_knn = estudo_knn.best_params\n",
    "melhores_params_rf = estudo_rf.best_params\n",
    "melhores_params_gb = estudo_gb.best_params\n",
    "melhores_params_en = estudo_en.best_params\n",
    "melhores_params_ridge = estudo_ridge.best_params\n",
    "\n",
    "pipeline_knn = Pipeline([\n",
    "    ('regressor', KNeighborsRegressor(**{k: v for k, v in melhores_params_knn.items() if k != 'regressor__'}))\n",
    "])\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('regressor', RandomForestRegressor(**{k: v for k, v in melhores_params_rf.items() if k != 'regressor__'}))\n",
    "])\n",
    "\n",
    "pipeline_gb = Pipeline([\n",
    "    ('regressor', GradientBoostingRegressor(**{k: v for k, v in melhores_params_gb.items() if k != 'regressor__'}, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_en = Pipeline([\n",
    "    ('regressor', ElasticNet(**{k: v for k, v in melhores_params_en.items() if k != 'regressor__'}, random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "pipeline_ridge = Pipeline([\n",
    "    ('regressor', Ridge(**{k: v for k, v in melhores_params_ridge.items() if k != 'regressor__'}, random_state=42, max_iter=10000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa37ba-d76d-4a9d-8d55-9d2ea012dbe9",
   "metadata": {},
   "source": [
    "---\n",
    "### **MODELOS OTIMIZADOS COM ANÁLISE DE DESEMPENHO DE VALIDAÇÃO CRUZADA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906957dc-aac6-4f62-a0e5-947476eb6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'KNN': pipeline_knn,\n",
    "    'Random Forest': pipeline_rf,\n",
    "    'Gradient Boosting': pipeline_gb,\n",
    "    'Elastic Net': pipeline_en,\n",
    "    'Ridge Regression': pipeline_ridge\n",
    "}\n",
    "\n",
    "resultados = {}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for nome, pipeline in pipelines.items():\n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=kf, scoring='neg_mean_squared_error')\n",
    "    scores_mse = -scores_cv\n",
    "    scores_rmse = np.sqrt(scores_mse)\n",
    "    \n",
    "    pipeline.fit(X_treino, y_treino)\n",
    "    y_previsto = pipeline.predict(X_teste)\n",
    "    rmse_teste = root_mean_squared_error(y_teste, y_previsto)\n",
    "    \n",
    "    resultados[nome] = {\n",
    "        'RMSE_CV_media': scores_rmse.mean(),\n",
    "        'RMSE_CV_desvio': scores_rmse.std(),\n",
    "        'RMSE_Teste': rmse_teste,\n",
    "    }\n",
    "    \n",
    "    print(f\"{nome}:\")\n",
    "    print(f\"RMSE Validação Cruzada: {scores_rmse.mean():.4f} (±{scores_rmse.std():.4f})\")\n",
    "    print(f\"RMSE Teste: {rmse_teste:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a38d6e-e947-4f86-a859-888cac012722",
   "metadata": {},
   "source": [
    "---\n",
    "### **EXPLICAÇÃO DOS MODELOS COM SHAP E DISCUSSÃO DE RESULTADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667da050-9bdb-484f-afff-01ae0a8f0d79",
   "metadata": {},
   "source": [
    "O SHAP (SHapley Additive exPlanations) é um método de interpretabilidade baseado na teoria dos jogos que oferece explicações locais para previsões individuais e análises globais para o comportamento geral do modelo. Enquanto as explicações locais atribuem a contribuição específica de cada feature para uma única previsão, o impacto global agrega essas contribuições para todo o dataset, identificando padrões dominantes e relações estruturais. Esta capacidade de transitar entre perspectivas micro e macro torna o SHAP particularmente versátil para compreender tanto casos específicos quanto tendências gerais do modelo.\n",
    "\n",
    "O funcionamento do SHAP baseia-se no cálculo da contribuição marginal média de cada feature, considerando todas as combinações possíveis de variáveis. Para análises locais, o método calcula exatamente como cada feature influenciou uma previsão específica. Já para o impacto global, o SHAP agrega esses valores individuais através de métricas como a média dos valores absolutos (mean |SHAP value|), criando um ranking robusto de importância de features. Esta abordagem garante que tanto a frequência quanto a magnitude do impacto de cada variável sejam consideradas, revelando interações complexas e dependências contextuais no modelo como um todo.\n",
    "\n",
    "No presente trabalho, usaremos o SHAP para analisar o comportamento geral de cada modelo neste dataset, a fim de descobrir como cada feature contribui. Para isso, vamos começar instalando o SHAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f9719-0926-4903-9ca0-68583d919bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: shap in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (0.49.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (2.3.0)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\program files\\python312\\lib\\site-packages (from shap) (24.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (0.62.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (4.14.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from numba>=0.54->shap) (0.45.1)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python312\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python312\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->shap) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->shap) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da4ffb-685c-4596-8886-bedd0bf1f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Em seguida, vamos ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4369d2ff-5ff9-4b92-b889-2491b5871b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelo_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m\n\u001b[0;32m     77\u001b[0m             plot_impacto_global(resultados_shap, feature_names, modelo_nome)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resultados_shaps\n\u001b[0;32m     81\u001b[0m modelos_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKNN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodelo_knn\u001b[49m,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: modelo_rf, \n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: modelo_gbr,\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElastic Net\u001b[39m\u001b[38;5;124m'\u001b[39m: modelo_en,\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRidge\u001b[39m\u001b[38;5;124m'\u001b[39m: modelo_rr\n\u001b[0;32m     87\u001b[0m }\n\u001b[0;32m     90\u001b[0m resultados, importancia_modelos \u001b[38;5;241m=\u001b[39m pipeline_shap_simplificada(\n\u001b[0;32m     91\u001b[0m     modelos_dict, X_treino, X_teste, FEATURES\n\u001b[0;32m     92\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modelo_knn' is not defined"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def executar_shap_todos_modelos(modelos_dict, X_treino, X_teste, feature_names, n_amostras=100):\n",
    "\n",
    "    resultados_shap = {}\n",
    "    \n",
    "    # Amostrar dados para tornar SHAP mais rápido\n",
    "    if len(X_teste) > n_amostras:\n",
    "        indices_amostra = np.random.choice(len(X_teste), n_amostras, replace=False)\n",
    "        X_teste_shap = X_teste.iloc[indices_amostra] if hasattr(X_teste, 'iloc') else X_teste[indices_amostra]\n",
    "    else:\n",
    "        X_teste_shap = X_teste\n",
    "    \n",
    "    for nome_modelo, modelo in modelos_dict.items():     \n",
    "        \n",
    "            # Selecionar explainer baseado no tipo de modelo\n",
    "            if 'Random Forest' in nome_modelo or 'Gradient Boosting' in nome_modelo:\n",
    "                explainer = shap.TreeExplainer(modelo)\n",
    "                shap_values = explainer.shap_values(X_teste_shap)\n",
    "                \n",
    "            elif 'KNN' in nome_modelo:\n",
    "                def predict_fn(X):\n",
    "                    return modelo.predict(X)\n",
    "                explainer = shap.KernelExplainer(predict_fn, X_treino[:100])\n",
    "                shap_values = explainer.shap_values(X_teste_shap)\n",
    "                \n",
    "            elif 'Elastic Net' in nome_modelo or 'Ridge' in nome_modelo:\n",
    "                explainer = shap.LinearExplainer(modelo, X_treino)\n",
    "                shap_values = explainer.shap_values(X_teste_shap)\n",
    "                \n",
    "            else:\n",
    "                explainer = shap.Explainer(modelo, X_treino)\n",
    "                shap_values = explainer.shap_values(X_teste_shap)\n",
    "            \n",
    "            # Armazenar resultados\n",
    "            resultados_shap[nome_modelo] = {\n",
    "                'explainer': explainer,\n",
    "                'shap_values': shap_values,\n",
    "                'expected_value': explainer.expected_value,\n",
    "                'X_teste_shap': X_teste_shap\n",
    "            }\n",
    "            \n",
    "    \n",
    "    return resultados_shap\n",
    "\n",
    "def plot_impacto_global(resultados_shap, feature_names, modelo_nome):\n",
    "\n",
    "    \n",
    "    if resultados_shap[modelo_nome] is None:\n",
    "        return\n",
    "        \n",
    "    dados = resultados_shap[modelo_nome]\n",
    "    shap_values = dados['shap_values']\n",
    "    X_teste_shap = dados['X_teste_shap']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_teste_shap, feature_names=feature_names, \n",
    "                      plot_type=\"bar\", show=False)\n",
    "    plt.title(f'Impacto Global - {modelo_nome}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def pipeline_shap_simplificada(modelos_dict, X_treino, X_teste, feature_names):\n",
    "    \n",
    "    # Calcular SHAP values para todos os modelos\n",
    "    resultados_shap = executar_shap_todos_modelos(modelos_dict, X_treino, X_teste, feature_names)\n",
    "    \n",
    "    # Gráficos de impacto global para cada modelo\n",
    "    for modelo_nome in modelos_dict.keys():\n",
    "        if resultados_shap[modelo_nome] is not None:\n",
    "            plot_impacto_global(resultados_shap, feature_names, modelo_nome)\n",
    "    \n",
    "    return resultados_shaps\n",
    "\n",
    "modelos_dict = {\n",
    "    'KNN': modelo_knn,\n",
    "    'Random Forest': modelo_rf, \n",
    "    'Gradient Boosting': modelo_gbr,\n",
    "    'Elastic Net': modelo_en,\n",
    "    'Ridge': modelo_rr\n",
    "}\n",
    "\n",
    "\n",
    "resultados, importancia_modelos = pipeline_shap_simplificada(\n",
    "    modelos_dict, X_treino, X_teste, FEATURES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e90a10-1f25-403e-8c6d-0fb05b25a996",
   "metadata": {},
   "source": [
    "---\n",
    "## **CONCLUSÃO**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a24d05-a31c-4595-974d-f65ec2d2b519",
   "metadata": {},
   "source": [
    "---\n",
    "## **INTRODUÇÃO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc15cc-03e2-47db-ac73-d86a6898cab7",
   "metadata": {},
   "source": [
    "---\n",
    "## **DESENVOLVIMENTO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a763f-3b88-4cd8-a80e-b0c12605922c",
   "metadata": {},
   "source": [
    "---\n",
    "### **DOWNLOAD DO DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58f7e9-e896-4f6e-b32b-50a2c95cbc06",
   "metadata": {},
   "source": [
    "Para iniciar nossa análise, utilizaremos o dataset \"Student Performance Factors\" disponível no Kaggle. Realizamos o download do conjunto de dados através do link  https://teams.microsoft.com/l/message/19:e98de34e8e034b56b694d6cdac057368@thread.v2/1760965275974?context=%7B%22contextType%22%3A%22chat%22%7D e extraímos o arquivo compactado em formato ZIP. O arquivo resultante, em formato CSV, foi armazenado no diretório da Quest 4 do projeto no Jupyter Notebook. Agora, procederemos com a leitura e carregamento dos dados para dar início ao processo de exploração e modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f67024-e231-4a5b-a8f9-bac780ae430f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Hours_Studied  Attendance Parental_Involvement Access_to_Resources  \\\n",
      "0                23          84                  Low                High   \n",
      "1                19          64                  Low              Medium   \n",
      "2                24          98               Medium              Medium   \n",
      "3                29          89                  Low              Medium   \n",
      "4                19          92               Medium              Medium   \n",
      "...             ...         ...                  ...                 ...   \n",
      "6602             25          69                 High              Medium   \n",
      "6603             23          76                 High              Medium   \n",
      "6604             20          90               Medium                 Low   \n",
      "6605             10          86                 High                High   \n",
      "6606             15          67               Medium                 Low   \n",
      "\n",
      "     Extracurricular_Activities  Sleep_Hours  Previous_Scores  \\\n",
      "0                            No            7               73   \n",
      "1                            No            8               59   \n",
      "2                           Yes            7               91   \n",
      "3                           Yes            8               98   \n",
      "4                           Yes            6               65   \n",
      "...                         ...          ...              ...   \n",
      "6602                         No            7               76   \n",
      "6603                         No            8               81   \n",
      "6604                        Yes            6               65   \n",
      "6605                        Yes            6               91   \n",
      "6606                        Yes            9               94   \n",
      "\n",
      "     Motivation_Level Internet_Access  Tutoring_Sessions Family_Income  \\\n",
      "0                 Low             Yes                  0           Low   \n",
      "1                 Low             Yes                  2        Medium   \n",
      "2              Medium             Yes                  2        Medium   \n",
      "3              Medium             Yes                  1        Medium   \n",
      "4              Medium             Yes                  3        Medium   \n",
      "...               ...             ...                ...           ...   \n",
      "6602           Medium             Yes                  1          High   \n",
      "6603           Medium             Yes                  3           Low   \n",
      "6604              Low             Yes                  3           Low   \n",
      "6605             High             Yes                  2           Low   \n",
      "6606           Medium             Yes                  0        Medium   \n",
      "\n",
      "     Teacher_Quality School_Type Peer_Influence  Physical_Activity  \\\n",
      "0             Medium      Public       Positive                  3   \n",
      "1             Medium      Public       Negative                  4   \n",
      "2             Medium      Public        Neutral                  4   \n",
      "3             Medium      Public       Negative                  4   \n",
      "4               High      Public        Neutral                  4   \n",
      "...              ...         ...            ...                ...   \n",
      "6602          Medium      Public       Positive                  2   \n",
      "6603            High      Public       Positive                  2   \n",
      "6604          Medium      Public       Negative                  2   \n",
      "6605          Medium     Private       Positive                  3   \n",
      "6606          Medium      Public       Positive                  4   \n",
      "\n",
      "     Learning_Disabilities Parental_Education_Level Distance_from_Home  \\\n",
      "0                       No              High School               Near   \n",
      "1                       No                  College           Moderate   \n",
      "2                       No             Postgraduate               Near   \n",
      "3                       No              High School           Moderate   \n",
      "4                       No                  College               Near   \n",
      "...                    ...                      ...                ...   \n",
      "6602                    No              High School               Near   \n",
      "6603                    No              High School               Near   \n",
      "6604                    No             Postgraduate               Near   \n",
      "6605                    No              High School                Far   \n",
      "6606                    No             Postgraduate               Near   \n",
      "\n",
      "      Gender  Exam_Score  \n",
      "0       Male          67  \n",
      "1     Female          61  \n",
      "2       Male          74  \n",
      "3       Male          71  \n",
      "4     Female          70  \n",
      "...      ...         ...  \n",
      "6602  Female          68  \n",
      "6603  Female          69  \n",
      "6604  Female          68  \n",
      "6605  Female          68  \n",
      "6606    Male          64  \n",
      "\n",
      "[6607 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('StudentPerformanceFactors.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce161e94-ffd8-441a-9c55-27acd8e03b4d",
   "metadata": {},
   "source": [
    "---\n",
    "### **TRATAMENTO DE DADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9432913-4f8c-4eb4-8d28-2f771e340aaa",
   "metadata": {},
   "source": [
    "No tratamento de dados, será realizado três passos:\n",
    "1. Retirar linhas que não tenham algum valor, para evitar erros no treinamento;\n",
    "2. Converter os dados categóricos (ordinais e binários) para numéricos, já que nosso target é numérico e usaremos regressão para o treinamento;\n",
    "3. Dividir os dados entre treino e teste para aplicar no modelo.\n",
    "   \n",
    "Vamos começar com o primeiro tópico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a55aea9b-d6ff-4a64-974f-4e0ac5edfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns #para carregar datasets prontos;\n",
    "import pandas #para manipulação de dados;\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler #para conversão de dados categóricos;\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop de colunas com QUALQUER linha NaN\n",
    "df = df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab735342-b6fd-4472-bb8b-c00ed728d319",
   "metadata": {},
   "source": [
    "#### **CONVERSÃO DE DADOS CATEGÓRICOS PARA NUMÉRICOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb16a6-1f5f-4098-b9f7-c787d894a10a",
   "metadata": {},
   "source": [
    "Abaixo segue o código que irá converter dados categóricos ordinais com o OrdinalEncoder, atribuindo valores numéricos correspondentes à ordem de cada categoria, e dados binários a partir de \"mapping\" cada label com 0 ou 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1d72b0-ba0c-4a55-a788-c3395bf615c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricos = {\n",
    "    'Parental_Involvement': ['Low', 'Medium', 'High'],\n",
    "    'Access_to_Resources': ['Low', 'Medium', 'High'],\n",
    "    'Motivation_Level': ['Low', 'Medium', 'High'],\n",
    "    'Family_Income': ['Low', 'Medium', 'High'],\n",
    "    'Teacher_Quality': ['Low', 'Medium', 'High'],\n",
    "    'Distance_from_Home': ['Near', 'Moderate', 'Far'],\n",
    "    'Peer_Influence': ['Negative', 'Neutral', 'Positive'],\n",
    "    'Parental_Education_Level': ['High School', 'College', 'Postgraduate'] \n",
    "}\n",
    "\n",
    "binarios = {\n",
    "    'Gender': {'Male': 0, 'Female': 1},\n",
    "    'Learning_Disabilities': {'No': 0, 'Yes': 1},\n",
    "    'Extracurricular_Activities': {'No': 0, 'Yes': 1},\n",
    "    'Internet_Access': {'No': 0, 'Yes': 1},\n",
    "    'School_Type': {'Public': 0, 'Private': 1}\n",
    "}\n",
    "\n",
    "for coluna, ordem in categoricos.items():\n",
    "    encoder = OrdinalEncoder(categories=[ordem])\n",
    "    df[coluna] = encoder.fit_transform(df[[coluna]])\n",
    "\n",
    "for coluna, mapa in binarios.items():\n",
    "    df[coluna] = df[coluna].map(mapa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97053719-6c95-4c4f-96c6-9acd0d3de81e",
   "metadata": {},
   "source": [
    "#### **SPLIT E NORMALIZAÇÃO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b783f2-9815-4d46-a452-d4454f658fd3",
   "metadata": {},
   "source": [
    "Nesta seção, iremos dividir nossa dataset entre treino e teste para poder criar e treinar os modelos. Além disso, vamos definir nossos atributos e o nosso target, que será o exam score. Com esses objetivos, vamos usar funções do `sklearn.model_selection`. Para a normalização, foi escolhido o `StandardScaler`.\n",
    "\n",
    "Note que a normalização com StandardScaler é crucial para KNN, Ridge Regression e Elastic Net devido à sua sensibilidade à escala dos dados. O KNN baseia-se em cálculos de distância onde features em escalas diferentes distorceriam as medidas de similaridade. Já Ridge e Elastic Net aplicam penalidades de regularização que seriam injustamente influenciadas por variáveis com magnitude naturalmente maior, comprometendo a eficácia do modelo. O StandardScaler padroniza os dados removendo a média e escalando para variância unitária, transformando cada feature para ter média zero e desvio padrão igual a 1. Essa transformação garante que todas as features contribuam igualmente para os algoritmos, independentemente de suas escalas originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271c1a24-a66a-4e0b-8501-b79ec966394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Hours_Studied', 'Attendance', 'Parental_Involvement',\n",
      "       'Access_to_Resources', 'Extracurricular_Activities', 'Sleep_Hours',\n",
      "       'Previous_Scores', 'Motivation_Level', 'Internet_Access',\n",
      "       'Tutoring_Sessions', 'Family_Income', 'Teacher_Quality', 'School_Type',\n",
      "       'Peer_Influence', 'Physical_Activity', 'Learning_Disabilities',\n",
      "       'Parental_Education_Level', 'Distance_from_Home', 'Gender',\n",
      "       'Exam_Score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2386ca9-ef92-4f64-ad31-afc632361213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #Para dividir entre treino e teste do modelo\n",
    "\n",
    "FEATURES = ['Hours_Studied', 'Attendance', 'Parental_Involvement',\n",
    "       'Access_to_Resources', 'Extracurricular_Activities', 'Sleep_Hours',\n",
    "       'Previous_Scores', 'Motivation_Level', 'Internet_Access',\n",
    "       'Tutoring_Sessions', 'Family_Income', 'Teacher_Quality', 'School_Type',\n",
    "       'Peer_Influence', 'Physical_Activity', 'Learning_Disabilities',\n",
    "       'Parental_Education_Level', 'Distance_from_Home', 'Gender']\n",
    "#definindo o alvo que será previsto\n",
    "TARGET = ['Exam_Score']\n",
    "    \n",
    "#definindo na variável X os valores de cada coluna que é parâmetro\n",
    "X = df[FEATURES].values\n",
    "    \n",
    "#definindo na variável y os valores da coluna alvo e transformando em unidimensional com o método \".ravel()\"\n",
    "y = df[TARGET].values.ravel()\n",
    "\n",
    "#dividindo entre teste e treino, com uma porcentagem de 20% para teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "normalizador = StandardScaler()\n",
    "X_treino_norm = normalizador.fit_transform(X_treino)\n",
    "X_teste_norm = normalizador.transform(X_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e099343-2504-48dc-970c-69be4dac0664",
   "metadata": {},
   "source": [
    "---\n",
    "### **BASELINE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a2fc3-f0d5-4d5e-a3db-883251d30c24",
   "metadata": {},
   "source": [
    "Nesta seção, estabeleceremos uma baseline de performance como referência fundamental para avaliar a eficácia dos nossos modelos de regressão. A baseline representa o desempenho mínimo que nossos algoritmos devem superar para demonstrar valor preditivo. Utilizaremos abordagens simples, prevendo constantemente a média dos valores de treino. Esta prática é crucial para validar que a complexidade adicional dos algoritmos de machine learning está de fato gerando ganhos preditivos significativos em relação a soluções triviais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf67fc0-5397-45f9-ba03-70308cc87c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9513102465608547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Calcula a média do target no treino\n",
    "media = np.mean(y_treino)\n",
    "\n",
    "# Cria previsões (sempre a média)\n",
    "y_pred_baseline = np.full_like(y_teste, fill_value=media)\n",
    "\n",
    "# Calcula RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_baseline)\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9e62d-3233-464b-9f15-d0d361cb9848",
   "metadata": {},
   "source": [
    "---\n",
    "### **CRIANDO O MODELO E TREINANDO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be9bf4-3349-4cb0-a1e2-25b2312f2cda",
   "metadata": {},
   "source": [
    "Nesta seção, aplicaremos cinco algoritmos de regressão distintos - K-Nearest Neighbors (KNN), Random Forest, Elastic Net, Gradient Boosting Regressor e Ridge Regression - para treinar modelos preditivos utilizando nosso conjunto de dados. Para cada algoritmo, iniciaremos com uma fundamentação teórica que explica seus princípios de funcionamento e características principais, seguida pela implementação prática utilizando a biblioteca scikit-learn.\n",
    "\n",
    "É importante destacar que, nesta fase inicial de modelagem, utilizaremos os valores padrão de hiperparâmetros definidos pelo scikit-learn para cada algoritmo. Esta abordagem nos permitirá estabelecer uma linha de base de performance antes de proceder com a otimização sistemática de hiperparâmetros, que será realizada em etapas subsequentes do projeto.\n",
    "\n",
    "Para avalisar o desempenho dos algoritmos neste primeiro momento, será usado o Root Mean Squared Error (RMSE) - uma métrica que quantifica a magnitude média dos erros de previsão. É a raiz quadrada da média dos quadrados das diferenças entre valores observados e previstos e está na mesma unidade da variável target. Por utilizar o quadrado dos erros, esta métrica penaliza mais fortemente previsões que estão significativamente distantes dos valores reais, sendo particularmente sensível a outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656795d-36a7-40f3-90e7-5ee4edc0d777",
   "metadata": {},
   "source": [
    "#### **K-nearest neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df28f8-4caf-467c-a465-e9e4b3d9ca69",
   "metadata": {},
   "source": [
    "O algoritmo K-Nearest Neighbors (KNN) é um método de aprendizado baseado em instâncias que realiza previsões calculando a similaridade entre observações no espaço de features. Quando aplicado a problemas de regressão, o KNN identifica os k pontos de dados mais próximos da observação que se deseja prever, utilizando métricas de distância como Euclidiana ou Manhattan, e então calcula a média dos valores target desses vizinhos para gerar a previsão final. Esta abordagem não constrói um modelo explícito durante o treinamento, mas armazena todo o conjunto de dados, realizando os cálculos apenas no momento da inferência, o que o torna computacionalmente intensivo para conjuntos \n",
    "grandes, porém flexível para capturar padrões complexos não-lineares nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c99474ad-f46f-4125-92dc-6112ae64221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.783189573246307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor #Para aplicar o modelo\n",
    "from sklearn.metrics import root_mean_squared_error #Para avaliar seu desempenho\n",
    " \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_treino_scaled = scaler.fit_transform(X_treino)\n",
    "X_teste_scaled = scaler.transform(X_teste)\n",
    "\n",
    "#criando modelo\n",
    "modelo_knn = KNeighborsRegressor()\n",
    "modelo_knn.fit(X_treino_scaled, y_treino)\n",
    "    \n",
    "#treinando o modelo\n",
    "y_pred_knn = modelo_knn.predict(X_teste_scaled)\n",
    "\n",
    "#avaliando o desempenho com RMSE (que será nosso retorno) a partir do target previsto\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_knn)\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c701e84-2ee9-41f6-8c3d-c2e782fad5b3",
   "metadata": {},
   "source": [
    "#### **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be016a-e5b7-4953-a50c-ea5928c9b0cf",
   "metadata": {},
   "source": [
    "Random Forest Regression é um método de ensemble baseado em bagging (Bootstrap Aggregating) que constrói múltiplas árvores de decisão independentes e combina suas previsões através da média. O bagging é uma técnica que reduz a variância do modelo através da criação de múltiplos conjuntos de treinamento via amostragem com reposição (bootstrap) e da agregação de seus resultados. Durante o treinamento, cada árvore é construída usando uma amostra bootstrap do conjunto de dados original e, em cada divisão de nó, apenas um subconjunto aleatório de features é considerado para seleção, introduzindo dupla aleatorização que promove diversidade entre as árvores e reduz significativamente o overfitting, resultando em um modelo robusto que geralmente apresenta excelente performance com pouca necessidade de ajuste fino de hiperparâmetros e natural resistência a outliers e ruídos nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788aefd8-b74c-4aed-8b8b-c6c65ba00031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4137802205209273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Criando modelo\n",
    "modelo_rf = RandomForestRegressor()\n",
    "    \n",
    "# Treinando o modelo\n",
    "modelo_rf.fit(X_treino, y_treino)\n",
    "    \n",
    "# Fazendo previsões\n",
    "y_pred_rf = modelo_rf.predict(X_teste)\n",
    "    \n",
    "# Avaliando o desempenho com RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_rf)\n",
    "    \n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952431d-bf44-4534-ab00-5e8f13616b09",
   "metadata": {},
   "source": [
    "#### **Gradient Boosting Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1062d3b-69ae-45f3-b7a4-2bd82555a46a",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression é uma técnica de ensemble sequencial que constrói um modelo preditivo forte através da combinação iterativa de múltiplos modelos fracos. O termo ensemble refere-se à abordagem que combina múltiplos modelos base para criar um predictor mais robusto e preciso do que qualquer modelo individual. O algoritmo opera ajustando sequencialmente novos modelos aos resíduos (erros) dos modelos anteriores, utilizando gradiente descendente para minimizar uma função de perda diferenciável, onde cada nova árvore é treinada para prever os gradientes negativos dos erros cometidos pelas árvores existentes, criando assim um processo de melhoria contínua que gradualmente reduz o viés do modelo enquanto mantém a variância controlada através de parâmetros como taxa de aprendizado e profundidade das árvores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0620f98c-0209-4394-8997-a3d2b0b64da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1534226663806635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Criando modelo\n",
    "modelo_gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo_gbr.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred_gbr = modelo_gbr.predict(X_teste)\n",
    "\n",
    "# Avaliando o desempenho com RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_gbr)\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdffa5-3b6d-49eb-9546-13abee2ee125",
   "metadata": {},
   "source": [
    "#### **Elastic Net**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a04dd1-a962-4741-9f58-55f8c13873bb",
   "metadata": {},
   "source": [
    "Elastic Net é um algoritmo de regressão regularizada que combina as penalidades L1 (Lasso) e L2 (Ridge) em uma única função objetivo. A penalidade L1 promove esparsidade nos coeficientes, efetivamente realizando seleção de variáveis ao zerar coeficientes de features menos importantes, enquanto a penalidade L2 contrai uniformemente todos os coeficientes para lidar com multicolinearidade. Durante o treinamento, o algoritmo adiciona um termo de penalização à função de custo tradicional que é uma combinação linear das normas L1 e L2 dos coeficientes, controlada pelo parâmetro alpha para a força total da regularização e l1_ratio para o balanço entre as duas penalidades, resultando em um modelo que pode efetivamente reduzir overfitting enquanto mantém a interpretabilidade através da criação de coeficientes exatamente zero para features irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "891931e6-de12-4b56-b619-44c56265cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9475110833146863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Criando modelo\n",
    "modelo_en = ElasticNet()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo_en.fit(X_treino_scaled, y_treino)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred_en = modelo_en.predict(X_teste_scaled)\n",
    "\n",
    "# Avaliando o desempenho com RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_en)\n",
    "    \n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c97245-4011-45bc-9730-3c9df5aa9487",
   "metadata": {},
   "source": [
    "#### **Ridge Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02803817-ff97-42e1-95a2-955ce857ceb5",
   "metadata": {},
   "source": [
    "Ridge Regression é uma extensão da regressão linear ordinária que adiciona uma penalidade L2 (soma dos quadrados dos coeficientes) à função de custo para lidar com problemas de multicolinearidade e overfitting. A penalidade L2, também conhecida como regularização de Tikhonov, adiciona o quadrado da magnitude dos coeficientes à função de custo, efetivamente contraindo todos os coeficientes proporcionalmente sem eliminá-los completamente. Ao incorporar este termo de regularização controlado pelo parâmetro alpha, o algoritmo estabiliza as estimativas na presença de features altamente correlacionadas e melhora a generalização do modelo, mantendo toda a interpretabilidade da regressão linear tradicional enquanto oferece melhor performance preditiva em situações onde a matriz de design é mal-condicionada ou o número de features é grande relativo ao número de observações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd3f61f-c6a2-4173-b5f8-b8567fe4ba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.037888341089658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_treino_scaled = scaler.fit_transform(X_treino)\n",
    "X_teste_scaled = scaler.transform(X_teste)\n",
    "\n",
    "# Criando modelo\n",
    "modelo_rr = Ridge()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo_rr.fit(X_treino_scaled, y_treino)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred_rr = modelo_rr.predict(X_teste_scaled)\n",
    "\n",
    "# Avaliando o desempenho com RMSE\n",
    "RMSE = root_mean_squared_error(y_teste, y_pred_rr)\n",
    "\n",
    "print(RMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab1857",
   "metadata": {},
   "source": [
    "---\n",
    "### **SELEÇÃO DE ATRIBUTOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d2d04",
   "metadata": {},
   "source": [
    "Dentro do dataset, há 19 features que podem ser utilizadas pelos algoritmos para tentar prever o valor do target final. Naturalmente, é de se esperar que algumas features tenham mais importância - isto é, influenciam mais no resultado final - e outras tenham menos. Nesse sentido, reduzir o número de features retirando aquelas que não possuem tanto peso no resultado final se mostra uma ótima estratégia.\n",
    "\n",
    "Reduzir o número de atributos tem dois motivos principais que serão cruciais para o andamento do código: melhoria do desempenho computacional e prevenção de overfitting. Muitos algoritmos - como o Random Forest, de $O(n \\cdot log\\,n)$ - possuem complexidade que escala muito rapidamente com o número de features. Diminuir a dimensionalidade nesse contexto aumenta a viabilidade de resolução de problemas, diminuindo tempo e o uso de recursos computacionais. Além disso, com features excessivas em relação ao número de amostras, os modelos tendem a memorizar ruídos e falsos padrões nos dados de treino ao invés de aprender relações generalizáveis.\n",
    "\n",
    "Para esse notebook, foi escolhido o atributo `feature_importance_`, usado para *ensembles* - como a Floresta Aleatória e o Gradient Boosting - e que retorna um array onde cada elemento dele é uma feature do modelo. Ele irá dizer, em pesos, o quão importante aquela feature é para o modelo. Vale ressaltar que o ideal é fazer a seleção de atributos especificamente para o algoritmo de ML que será utilizado, no entanto, optou-se por uma simplificação desse processo ao escolher apenas um processo de seleção de atributos.\n",
    "\n",
    "O código abaixo gerou um DataFrame de saída, com a feature e sua respectiva importância, sendo esta ordenada do maior para o menor a fim de listar os 10 atributos que mais influenciam no algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd6c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     feature  importance\n",
      "1                 Attendance    0.379373\n",
      "0              Hours_Studied    0.244850\n",
      "6            Previous_Scores    0.086903\n",
      "9          Tutoring_Sessions    0.034246\n",
      "3        Access_to_Resources    0.033835\n",
      "2       Parental_Involvement    0.032872\n",
      "14         Physical_Activity    0.027515\n",
      "5                Sleep_Hours    0.024340\n",
      "10             Family_Income    0.020852\n",
      "16  Parental_Education_Level    0.016734\n"
     ]
    }
   ],
   "source": [
    "importancia = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "importancia.fit(X_treino_norm, y_treino)\n",
    "importancia_features = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': importancia.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(importancia_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f91d6",
   "metadata": {},
   "source": [
    "Tendo estabelecido os atributos com maior fator de importância, colocaremos-os em uma lista para que se possa fazer o split novamente, dessa vez com menos features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c351e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_features = importancia_features.head(10)['feature'].tolist()\n",
    "\n",
    "X = df[top_10_features]\n",
    "y = df[TARGET].values.ravel()\n",
    "\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "normalizador = StandardScaler()\n",
    "X_treino_norm = normalizador.fit_transform(X_treino)\n",
    "X_teste_norm = normalizador.transform(X_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024501b0-3f00-45e6-8c90-6cef9ddbfa87",
   "metadata": {},
   "source": [
    "---\n",
    "### **OTIMIZAÇÃO DE HIPERPARÂMETROS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94cbb2",
   "metadata": {},
   "source": [
    "Nessa seção, busca-se estabelecer a melhor combinação de termos para potencializar o modelo. A otimização de hiperparâmetros é basicamente a criação de funções objetivo - uma expressão matemática que representa o problema a ser otimizado - especializadas para cada algoritmo, seguindo um padrão consistente de implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "632582c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b82e7",
   "metadata": {},
   "source": [
    "Para isso, cada função específica utiliza o objeto `trial` do Optuna para definir um espaço de busca multidimensional, onde diferentes combinações de hiperparâmetros são sistematicamente exploradas e avaliadas. Cada algoritmo é colocado em uma `Pipeline`, proporcionando uma interface mais organizada para definir cada parâmetro a ser testado e avaliado pelo Optuna, que utiliza como métrica a função `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "467417ba-141b-43fd-8ea0-68222031bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_knn(trial):\n",
    "    n_vizinhos = trial.suggest_int('n_neighbors', 1, 50)\n",
    "    pesos = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    p = trial.suggest_int('p', 1, 2)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', KNeighborsRegressor(\n",
    "            n_neighbors=n_vizinhos,\n",
    "            weights=pesos,\n",
    "            p=p\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "973cdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_rf(trial):\n",
    "    n_estimadores = trial.suggest_int('n_estimators', 50, 500)\n",
    "    profundidade_maxima = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_amostras_divisao = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_amostras_folha = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_caracteristicas = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators=n_estimadores,\n",
    "            max_depth=profundidade_maxima,\n",
    "            min_samples_split=min_amostras_divisao,\n",
    "            min_samples_leaf=min_amostras_folha,\n",
    "            max_features=max_caracteristicas,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af2037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_gb(trial):\n",
    "    n_estimadores = trial.suggest_int('n_estimators', 50, 500)\n",
    "    taxa_aprendizado = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    profundidade_maxima = trial.suggest_int('max_depth', 3, 10)\n",
    "    min_amostras_divisao = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_amostras_folha = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    sub_amostra = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', GradientBoostingRegressor(\n",
    "            n_estimators=n_estimadores,\n",
    "            learning_rate=taxa_aprendizado,\n",
    "            max_depth=profundidade_maxima,\n",
    "            min_samples_split=min_amostras_divisao,\n",
    "            min_samples_leaf=min_amostras_folha,\n",
    "            subsample=sub_amostra,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0485ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_elasticnet(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.001, 10.0, log=True)\n",
    "    razao_l1 = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', ElasticNet(\n",
    "            alpha=alpha,\n",
    "            l1_ratio=razao_l1,\n",
    "            random_state=42,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ba3db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objetivo_ridge(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.001, 10.0, log=True)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('regressor', Ridge(\n",
    "            alpha=alpha,\n",
    "            random_state=42,\n",
    "            max_iter=10000\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores_cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972aeca0",
   "metadata": {},
   "source": [
    "Após a criação das funções, deve-se iniciar estudos individuais para cada algoritmo por meio do atributo `create_study`, buscando maximizar a pontuação do modelo, a fim de buscar um modelo mais robusto. Cada estudo executa múltiplos trials, onde cada trial representa uma combinação específica de hiperparâmetros. Ao final do processo, os melhores parâmetros são identificados e serão utilizados para treinar o modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f354df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 21:44:29,875] A new study created in memory with name: no-name-86019ecc-60c9-4b01-94ab-8e837f99628d\n",
      "[I 2025-11-03 21:44:30,073] Trial 0 finished with value: -6.483037051614879 and parameters: {'n_neighbors': 38, 'weights': 'distance', 'p': 1}. Best is trial 0 with value: -6.483037051614879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a otimização do modelo KNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 21:44:30,199] Trial 1 finished with value: -6.729812699167115 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'p': 2}. Best is trial 0 with value: -6.483037051614879.\n",
      "[I 2025-11-03 21:44:30,405] Trial 2 finished with value: -6.5504865710533595 and parameters: {'n_neighbors': 48, 'weights': 'uniform', 'p': 1}. Best is trial 0 with value: -6.483037051614879.\n",
      "[I 2025-11-03 21:44:30,468] Trial 3 finished with value: -11.315051372164929 and parameters: {'n_neighbors': 1, 'weights': 'uniform', 'p': 2}. Best is trial 0 with value: -6.483037051614879.\n",
      "[I 2025-11-03 21:44:30,595] Trial 4 finished with value: -7.683929706896244 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'p': 1}. Best is trial 0 with value: -6.483037051614879.\n",
      "[I 2025-11-03 21:44:30,756] Trial 5 finished with value: -6.495116745063575 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'p': 1}. Best is trial 0 with value: -6.483037051614879.\n",
      "[I 2025-11-03 21:44:30,920] Trial 6 finished with value: -6.521252198187199 and parameters: {'n_neighbors': 24, 'weights': 'uniform', 'p': 1}. Best is trial 0 with value: -6.483037051614879.\n",
      "[I 2025-11-03 21:44:31,081] Trial 7 finished with value: -6.502559192139148 and parameters: {'n_neighbors': 28, 'weights': 'uniform', 'p': 1}. Best is trial 0 with value: -6.483037051614879.\n",
      "[I 2025-11-03 21:44:31,230] Trial 8 finished with value: -6.467011621327396 and parameters: {'n_neighbors': 37, 'weights': 'distance', 'p': 1}. Best is trial 8 with value: -6.467011621327396.\n",
      "[I 2025-11-03 21:44:31,403] Trial 9 finished with value: -6.536402104859967 and parameters: {'n_neighbors': 43, 'weights': 'uniform', 'p': 1}. Best is trial 8 with value: -6.467011621327396.\n",
      "[I 2025-11-03 21:44:31,533] Trial 10 finished with value: -6.691669207281665 and parameters: {'n_neighbors': 34, 'weights': 'distance', 'p': 2}. Best is trial 8 with value: -6.467011621327396.\n",
      "[I 2025-11-03 21:44:31,717] Trial 11 finished with value: -6.467011621327396 and parameters: {'n_neighbors': 37, 'weights': 'distance', 'p': 1}. Best is trial 8 with value: -6.467011621327396.\n",
      "[I 2025-11-03 21:44:31,916] Trial 12 finished with value: -6.472864581192131 and parameters: {'n_neighbors': 33, 'weights': 'distance', 'p': 1}. Best is trial 8 with value: -6.467011621327396.\n",
      "[I 2025-11-03 21:44:32,100] Trial 13 finished with value: -6.549426595365152 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'p': 1}. Best is trial 8 with value: -6.467011621327396.\n",
      "[I 2025-11-03 21:44:32,235] Trial 14 finished with value: -6.720682943667411 and parameters: {'n_neighbors': 39, 'weights': 'distance', 'p': 2}. Best is trial 8 with value: -6.467011621327396.\n",
      "[I 2025-11-03 21:44:32,387] Trial 15 finished with value: -6.604965949676869 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'p': 1}. Best is trial 8 with value: -6.467011621327396.\n",
      "[I 2025-11-03 21:44:32,567] Trial 16 finished with value: -6.46058816365139 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:32,685] Trial 17 finished with value: -6.702900144185598 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'p': 2}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:32,836] Trial 18 finished with value: -6.580257109203086 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:32,988] Trial 19 finished with value: -6.488189455713058 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:33,133] Trial 20 finished with value: -6.720935784877847 and parameters: {'n_neighbors': 42, 'weights': 'distance', 'p': 2}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:33,322] Trial 21 finished with value: -6.46405815317348 and parameters: {'n_neighbors': 35, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:33,510] Trial 22 finished with value: -6.479721543552051 and parameters: {'n_neighbors': 31, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:33,685] Trial 23 finished with value: -6.468157492914969 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:33,856] Trial 24 finished with value: -6.46405815317348 and parameters: {'n_neighbors': 35, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:34,055] Trial 25 finished with value: -6.479180105940074 and parameters: {'n_neighbors': 44, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:34,234] Trial 26 finished with value: -6.519862123309276 and parameters: {'n_neighbors': 34, 'weights': 'uniform', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:34,400] Trial 27 finished with value: -6.46058816365139 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:34,570] Trial 28 finished with value: -6.499494235364148 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'p': 1}. Best is trial 16 with value: -6.46058816365139.\n",
      "[I 2025-11-03 21:44:34,754] Trial 29 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:34,916] Trial 30 finished with value: -6.463303147638453 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:35,086] Trial 31 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:35,289] Trial 32 finished with value: -6.46763918987215 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:35,453] Trial 33 finished with value: -6.499494235364148 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:35,619] Trial 34 finished with value: -6.4657654735102685 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:35,815] Trial 35 finished with value: -6.505691598805081 and parameters: {'n_neighbors': 29, 'weights': 'uniform', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:35,902] Trial 36 finished with value: -6.962226334064913 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'p': 2}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:36,097] Trial 37 finished with value: -6.494678938101616 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:36,270] Trial 38 finished with value: -6.528792066490271 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:36,442] Trial 39 finished with value: -6.562358918817573 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:36,639] Trial 40 finished with value: -6.526816935118086 and parameters: {'n_neighbors': 39, 'weights': 'uniform', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:36,808] Trial 41 finished with value: -6.4657654735102685 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:36,968] Trial 42 finished with value: -6.463303147638453 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:37,148] Trial 43 finished with value: -6.479721543552051 and parameters: {'n_neighbors': 31, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:37,315] Trial 44 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:37,472] Trial 45 finished with value: -6.475326621521714 and parameters: {'n_neighbors': 32, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:37,661] Trial 46 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:37,811] Trial 47 finished with value: -6.531195915767173 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:37,952] Trial 48 finished with value: -6.488189455713058 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:38,101] Trial 49 finished with value: -6.507487683997151 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:38,248] Trial 50 finished with value: -6.467011621327396 and parameters: {'n_neighbors': 37, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:38,414] Trial 51 finished with value: -6.46058816365139 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:38,565] Trial 52 finished with value: -6.475326621521714 and parameters: {'n_neighbors': 32, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:38,715] Trial 53 finished with value: -6.477594500117979 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:38,885] Trial 54 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:38,982] Trial 55 finished with value: -6.699533620823807 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 2}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:39,135] Trial 56 finished with value: -6.472864581192131 and parameters: {'n_neighbors': 33, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:39,265] Trial 57 finished with value: -6.495116745063575 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:39,434] Trial 58 finished with value: -6.46405815317348 and parameters: {'n_neighbors': 35, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:39,582] Trial 59 finished with value: -6.4657654735102685 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:39,732] Trial 60 finished with value: -6.468157492914969 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:39,882] Trial 61 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:40,036] Trial 62 finished with value: -6.479721543552051 and parameters: {'n_neighbors': 31, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:40,184] Trial 63 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:40,349] Trial 64 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:40,482] Trial 65 finished with value: -6.488189455713058 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:40,632] Trial 66 finished with value: -6.46763918987215 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:40,782] Trial 67 finished with value: -6.472864581192131 and parameters: {'n_neighbors': 33, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:40,948] Trial 68 finished with value: -6.507487683997151 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:41,082] Trial 69 finished with value: -6.468157492914969 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:41,203] Trial 70 finished with value: -6.495116745063575 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:41,374] Trial 71 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:41,515] Trial 72 finished with value: -6.477594500117979 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:41,649] Trial 73 finished with value: -6.46058816365139 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:41,815] Trial 74 finished with value: -6.479721543552051 and parameters: {'n_neighbors': 31, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:41,948] Trial 75 finished with value: -6.463303147638453 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:42,082] Trial 76 finished with value: -6.47278077879057 and parameters: {'n_neighbors': 34, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:42,232] Trial 77 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:42,332] Trial 78 finished with value: -6.702900144185598 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'p': 2}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:42,487] Trial 79 finished with value: -6.508648173455474 and parameters: {'n_neighbors': 25, 'weights': 'uniform', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:42,591] Trial 80 finished with value: -7.436961562598315 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:42,734] Trial 81 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:42,882] Trial 82 finished with value: -6.477594500117979 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:43,032] Trial 83 finished with value: -6.475326621521714 and parameters: {'n_neighbors': 32, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:43,171] Trial 84 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:43,306] Trial 85 finished with value: -6.468157492914969 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:43,448] Trial 86 finished with value: -6.46763918987215 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:43,582] Trial 87 finished with value: -6.46058816365139 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:43,734] Trial 88 finished with value: -6.484742432446289 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:43,882] Trial 89 finished with value: -6.469226734194353 and parameters: {'n_neighbors': 36, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:44,034] Trial 90 finished with value: -6.472864581192131 and parameters: {'n_neighbors': 33, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:44,165] Trial 91 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:44,308] Trial 92 finished with value: -6.463303147638453 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:44,449] Trial 93 finished with value: -6.479721543552051 and parameters: {'n_neighbors': 31, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:44,582] Trial 94 finished with value: -6.458879881060199 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:44,715] Trial 95 finished with value: -6.468157492914969 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:44,869] Trial 96 finished with value: -6.514639375510788 and parameters: {'n_neighbors': 30, 'weights': 'uniform', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:45,015] Trial 97 finished with value: -6.475326621521714 and parameters: {'n_neighbors': 32, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:45,148] Trial 98 finished with value: -6.488189455713058 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:45,298] Trial 99 finished with value: -6.477594500117979 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'p': 1}. Best is trial 29 with value: -6.458879881060199.\n",
      "[I 2025-11-03 21:44:45,298] A new study created in memory with name: no-name-2a5f5b74-907a-4ebf-90fa-001b3d232078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a otimização do modelo Floresta Aleatória...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 21:44:49,066] Trial 0 finished with value: -6.102389679656065 and parameters: {'n_estimators': 386, 'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: -6.102389679656065.\n",
      "[I 2025-11-03 21:44:52,678] Trial 1 finished with value: -5.886212790636951 and parameters: {'n_estimators': 133, 'max_depth': 10, 'min_samples_split': 16, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 1 with value: -5.886212790636951.\n",
      "[I 2025-11-03 21:44:57,331] Trial 2 finished with value: -5.85407460663535 and parameters: {'n_estimators': 406, 'max_depth': 16, 'min_samples_split': 15, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 2 with value: -5.85407460663535.\n",
      "[I 2025-11-03 21:44:59,492] Trial 3 finished with value: -5.7875567853186 and parameters: {'n_estimators': 68, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 3 with value: -5.7875567853186.\n",
      "[I 2025-11-03 21:45:01,731] Trial 4 finished with value: -5.738706500727669 and parameters: {'n_estimators': 163, 'max_depth': 16, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 4 with value: -5.738706500727669.\n",
      "[I 2025-11-03 21:45:04,971] Trial 5 finished with value: -5.788157406012752 and parameters: {'n_estimators': 114, 'max_depth': 15, 'min_samples_split': 13, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 4 with value: -5.738706500727669.\n",
      "[I 2025-11-03 21:45:08,391] Trial 6 finished with value: -6.757791539332294 and parameters: {'n_estimators': 226, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 4 with value: -5.738706500727669.\n",
      "[I 2025-11-03 21:45:10,903] Trial 7 finished with value: -5.832583320753019 and parameters: {'n_estimators': 210, 'max_depth': 10, 'min_samples_split': 12, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 4 with value: -5.738706500727669.\n",
      "[I 2025-11-03 21:45:14,397] Trial 8 finished with value: -6.299220210763913 and parameters: {'n_estimators': 385, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 4 with value: -5.738706500727669.\n",
      "[I 2025-11-03 21:45:19,921] Trial 9 finished with value: -5.6561194724159485 and parameters: {'n_estimators': 379, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 9 with value: -5.6561194724159485.\n",
      "[I 2025-11-03 21:45:25,104] Trial 10 finished with value: -5.964961522137469 and parameters: {'n_estimators': 472, 'max_depth': 19, 'min_samples_split': 7, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 9 with value: -5.6561194724159485.\n",
      "[I 2025-11-03 21:45:28,830] Trial 11 finished with value: -5.724012012372417 and parameters: {'n_estimators': 278, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 9 with value: -5.6561194724159485.\n",
      "[I 2025-11-03 21:45:32,346] Trial 12 finished with value: -5.836386052795845 and parameters: {'n_estimators': 320, 'max_depth': 20, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 9 with value: -5.6561194724159485.\n",
      "[I 2025-11-03 21:45:36,446] Trial 13 finished with value: -5.723721530141648 and parameters: {'n_estimators': 310, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 9 with value: -5.6561194724159485.\n",
      "[I 2025-11-03 21:45:40,549] Trial 14 finished with value: -5.759105626335725 and parameters: {'n_estimators': 336, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 9 with value: -5.6561194724159485.\n",
      "[I 2025-11-03 21:45:47,912] Trial 15 finished with value: -5.657311601160005 and parameters: {'n_estimators': 464, 'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 9 with value: -5.6561194724159485.\n",
      "[I 2025-11-03 21:45:54,761] Trial 16 finished with value: -5.662365923911291 and parameters: {'n_estimators': 496, 'max_depth': 17, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 9 with value: -5.6561194724159485.\n",
      "[I 2025-11-03 21:46:01,506] Trial 17 finished with value: -5.655595510011112 and parameters: {'n_estimators': 436, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 17 with value: -5.655595510011112.\n",
      "[I 2025-11-03 21:46:07,945] Trial 18 finished with value: -5.659228780150412 and parameters: {'n_estimators': 436, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 17 with value: -5.655595510011112.\n",
      "[I 2025-11-03 21:46:12,664] Trial 19 finished with value: -5.759897049489423 and parameters: {'n_estimators': 358, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 17 with value: -5.655595510011112.\n",
      "[I 2025-11-03 21:46:15,014] Trial 20 finished with value: -8.872930954955734 and parameters: {'n_estimators': 417, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 17 with value: -5.655595510011112.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando a otimização do modelo KNN...\")\n",
    "estudo_knn = optuna.create_study(direction='maximize')\n",
    "estudo_knn.optimize(objetivo_knn, n_trials=100)\n",
    "\n",
    "print(\"Iniciando a otimização do modelo Floresta Aleatória...\")\n",
    "estudo_rf = optuna.create_study(direction='maximize')\n",
    "estudo_rf.optimize(objetivo_rf, n_trials=100)\n",
    "\n",
    "print(\"Iniciando a otimização do modelo Gradient Boosting...\")\n",
    "estudo_gb = optuna.create_study(direction='maximize')\n",
    "estudo_gb.optimize(objetivo_gb, n_trials=100)\n",
    "\n",
    "print(\"Iniciando a otimização do modelo Elastic Net...\")\n",
    "estudo_en = optuna.create_study(direction='maximize')\n",
    "estudo_en.optimize(objetivo_elasticnet, n_trials=100)\n",
    "\n",
    "print(\"Iniciando a otimização do modelo Ridge Regression...\")\n",
    "estudo_ridge = optuna.create_study(direction='maximize')\n",
    "estudo_ridge.optimize(objetivo_ridge, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1a078",
   "metadata": {},
   "source": [
    "Terminado esse processo, obtém-se os melhores parâmetros com o atributo `best_params` e declara-se o trial com o melhor desempenho na otimização. Com isso, novamente cria-se pipelines de cada algoritmo, definindo o método de regressão juntamente de uma expressão cujo objetivo é iterar sobre todos os pares \"chave-valor\" do dicionário de melhores parâmetros; filtrá-lo, removendo qualquer chave que inicie com `regressor__`, que causa problemas posteriomente se não for retirado; e, por fim, desempacotar esse dicionário usando o `**` no início da expressão para passar os parâmetros para o construtor do regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff68c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "melhores_params_knn = estudo_knn.best_params\n",
    "melhores_params_rf = estudo_rf.best_params\n",
    "melhores_params_gb = estudo_gb.best_params\n",
    "melhores_params_en = estudo_en.best_params\n",
    "melhores_params_ridge = estudo_ridge.best_params\n",
    "\n",
    "pipeline_knn = Pipeline([\n",
    "    ('regressor', KNeighborsRegressor(**{k: v for k, v in melhores_params_knn.items() if k != 'regressor__'}))\n",
    "])\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('regressor', RandomForestRegressor(**{k: v for k, v in melhores_params_rf.items() if k != 'regressor__'}))\n",
    "])\n",
    "\n",
    "pipeline_gb = Pipeline([\n",
    "    ('regressor', GradientBoostingRegressor(**{k: v for k, v in melhores_params_gb.items() if k != 'regressor__'}, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_en = Pipeline([\n",
    "    ('regressor', ElasticNet(**{k: v for k, v in melhores_params_en.items() if k != 'regressor__'}, random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "pipeline_ridge = Pipeline([\n",
    "    ('regressor', Ridge(**{k: v for k, v in melhores_params_ridge.items() if k != 'regressor__'}, random_state=42, max_iter=10000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa37ba-d76d-4a9d-8d55-9d2ea012dbe9",
   "metadata": {},
   "source": [
    "---\n",
    "### **MODELOS OTIMIZADOS COM ANÁLISE DE DESEMPENHO POR VALIDAÇÃO CRUZADA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4d7a9",
   "metadata": {},
   "source": [
    "Nesse tópico, foi feita a última verificação de desempenho dos cinco modelos escolhidos. Para tal, selecionou-se a validação cruzada, essencial para obter uma avaliação mais confiável do desempenho dos modelos. Diferente de uma simples divisão treino/teste - que por obra do acaso pode ser influenciada por uma divisão específica dos dados - a validação cruzada usa múltiplas divisões, onde cada parte dos dados serve tanto para treino quanto para teste em rodadas alternadas, os \"folds\".\n",
    "\n",
    "Isso fornece uma estimativa mais robusta da performance real do modelo, permitindo identificar melhor seu verdadeiro poder de generalização para novos dados. Ademais, também revela a consistência do modelo através da variação entre as rodadas e maximiza o uso dos dados disponíveis, especialmente importante em conjuntos menores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfb0b8",
   "metadata": {},
   "source": [
    "Para aplicar a validação cruzada, primeiro cria-se um dicionário que mapeia nomes às pipelines previamente configuradas com seus melhores parâmetros. Posteriormente, define-se o método de validação cruzada, que aqui é o `KFold`, e a quantidade de folds desejados, que tradicionalmente é 5 por ser um equilíbrio entre robustez e custo computacional.\n",
    "\n",
    "Após esses processos, declara-se um loop onde para cada modelo no dicionário de pipelines, onde executa-se a validação cruzada calculando o MSE negativo que é convertido para MSE positivo e partir dele, calcula-se o RMSE. Depois é feito o treinamento, o teste e calcula-se o RMSE no conjunto de teste. Para finalizar, os resultados são armazenados e imprimidos para cada algoritmo testado, mostrando tanto a performance na validação cruzada, quanto no teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906957dc-aac6-4f62-a0e5-947476eb6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'KNN': pipeline_knn,\n",
    "    'Random Forest': pipeline_rf,\n",
    "    'Gradient Boosting': pipeline_gb,\n",
    "    'Elastic Net': pipeline_en,\n",
    "    'Ridge Regression': pipeline_ridge\n",
    "}\n",
    "\n",
    "resultados = {}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for nome, pipeline in pipelines.items():\n",
    "    scores_cv = cross_val_score(pipeline, X_treino, y_treino, \n",
    "                              cv=kf, scoring='neg_mean_squared_error')\n",
    "    scores_mse = -scores_cv\n",
    "    scores_rmse = np.sqrt(scores_mse)\n",
    "    \n",
    "    pipeline.fit(X_treino, y_treino)\n",
    "    y_previsto = pipeline.predict(X_teste)\n",
    "    rmse_teste = root_mean_squared_error(y_teste, y_previsto)\n",
    "    \n",
    "    resultados[nome] = {\n",
    "        'RMSE_CV_media': scores_rmse.mean(),\n",
    "        'RMSE_CV_desvio': scores_rmse.std(),\n",
    "        'RMSE_Teste': rmse_teste,\n",
    "    }\n",
    "    \n",
    "    print(f\"{nome}:\")\n",
    "    print(f\"RMSE Validação Cruzada: {scores_rmse.mean():.4f} (±{scores_rmse.std():.4f})\")\n",
    "    print(f\"RMSE Teste: {rmse_teste:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a38d6e-e947-4f86-a859-888cac012722",
   "metadata": {},
   "source": [
    "---\n",
    "### **EXPLICAÇÃO DOS MODELOS COM SHAP E DISCUSSÃO DE RESULTADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667da050-9bdb-484f-afff-01ae0a8f0d79",
   "metadata": {},
   "source": [
    "O SHAP (SHapley Additive exPlanations) é um método de interpretabilidade baseado na teoria dos jogos que oferece explicações locais para previsões individuais e análises globais para o comportamento geral do modelo. Enquanto as explicações locais atribuem a contribuição específica de cada feature para uma única previsão, o impacto global agrega essas contribuições para todo o dataset, identificando padrões dominantes e relações estruturais. Esta capacidade de transitar entre perspectivas micro e macro torna o SHAP particularmente versátil para compreender tanto casos específicos quanto tendências gerais do modelo.\n",
    "\n",
    "O funcionamento do SHAP baseia-se no cálculo da contribuição marginal média de cada feature, considerando todas as combinações possíveis de variáveis. Para análises locais, o método calcula exatamente como cada feature influenciou uma previsão específica. Já para o impacto global, o SHAP agrega esses valores individuais através de métricas como a média dos valores absolutos (mean |SHAP value|), criando um ranking robusto de importância de features. Esta abordagem garante que tanto a frequência quanto a magnitude do impacto de cada variável sejam consideradas, revelando interações complexas e dependências contextuais no modelo como um todo.\n",
    "\n",
    "No presente trabalho, usaremos o SHAP para analisar o comportamento geral de cada modelo neste dataset, a fim de descobrir como cada feature contribui. Para isso, vamos começar instalando o SHAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f9719-0926-4903-9ca0-68583d919bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: shap in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (0.49.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (2.3.0)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\program files\\python312\\lib\\site-packages (from shap) (24.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (0.62.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from shap) (4.14.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from numba>=0.54->shap) (0.45.1)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python312\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python312\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->shap) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\giovani25025\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->shap) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da4ffb-685c-4596-8886-bedd0bf1f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Em seguida, vamos ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369d2ff-5ff9-4b92-b889-2491b5871b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelo_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m\n\u001b[0;32m     77\u001b[0m             plot_impacto_global(resultados_shap, feature_names, modelo_nome)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resultados_shaps\n\u001b[0;32m     81\u001b[0m modelos_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKNN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodelo_knn\u001b[49m,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: modelo_rf, \n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: modelo_gbr,\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElastic Net\u001b[39m\u001b[38;5;124m'\u001b[39m: modelo_en,\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRidge\u001b[39m\u001b[38;5;124m'\u001b[39m: modelo_rr\n\u001b[0;32m     87\u001b[0m }\n\u001b[0;32m     90\u001b[0m resultados, importancia_modelos \u001b[38;5;241m=\u001b[39m pipeline_shap_simplificada(\n\u001b[0;32m     91\u001b[0m     modelos_dict, X_treino, X_teste, FEATURES\n\u001b[0;32m     92\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modelo_knn' is not defined"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def executar_shap_todos_modelos(modelos_dict, X_treino, X_teste, feature_names, n_amostras=100):\n",
    "\n",
    "    resultados_shap = {}\n",
    "    \n",
    "    # Amostrar dados para tornar SHAP mais rápido\n",
    "    if len(X_teste) > n_amostras:\n",
    "        indices_amostra = np.random.choice(len(X_teste), n_amostras, replace=False)\n",
    "        X_teste_shap = X_teste.iloc[indices_amostra] if hasattr(X_teste, 'iloc') else X_teste[indices_amostra]\n",
    "    else:\n",
    "        X_teste_shap = X_teste\n",
    "    \n",
    "    for nome_modelo, modelo in modelos_dict.items():     \n",
    "        \n",
    "            # Selecionar explainer baseado no tipo de modelo\n",
    "            if 'Random Forest' in nome_modelo or 'Gradient Boosting' in nome_modelo:\n",
    "                explainer = shap.TreeExplainer(modelo)\n",
    "                shap_values = explainer.shap_values(X_teste_shap)\n",
    "                \n",
    "            elif 'KNN' in nome_modelo:\n",
    "                def predict_fn(X):\n",
    "                    return modelo.predict(X)\n",
    "                explainer = shap.KernelExplainer(predict_fn, X_treino[:100])\n",
    "                shap_values = explainer.shap_values(X_teste_shap)\n",
    "                \n",
    "            elif 'Elastic Net' in nome_modelo or 'Ridge' in nome_modelo:\n",
    "                explainer = shap.LinearExplainer(modelo, X_treino)\n",
    "                shap_values = explainer.shap_values(X_teste_shap)\n",
    "                \n",
    "            else:\n",
    "                explainer = shap.Explainer(modelo, X_treino)\n",
    "                shap_values = explainer.shap_values(X_teste_shap)\n",
    "            \n",
    "            # Armazenar resultados\n",
    "            resultados_shap[nome_modelo] = {\n",
    "                'explainer': explainer,\n",
    "                'shap_values': shap_values,\n",
    "                'expected_value': explainer.expected_value,\n",
    "                'X_teste_shap': X_teste_shap\n",
    "            }\n",
    "            \n",
    "    \n",
    "    return resultados_shap\n",
    "\n",
    "def plot_impacto_global(resultados_shap, feature_names, modelo_nome):\n",
    "\n",
    "    \n",
    "    if resultados_shap[modelo_nome] is None:\n",
    "        return\n",
    "        \n",
    "    dados = resultados_shap[modelo_nome]\n",
    "    shap_values = dados['shap_values']\n",
    "    X_teste_shap = dados['X_teste_shap']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_teste_shap, feature_names=feature_names, \n",
    "                      plot_type=\"bar\", show=False)\n",
    "    plt.title(f'Impacto Global - {modelo_nome}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def pipeline_shap_simplificada(modelos_dict, X_treino, X_teste, feature_names):\n",
    "    \n",
    "    # Calcular SHAP values para todos os modelos\n",
    "    resultados_shap = executar_shap_todos_modelos(modelos_dict, X_treino, X_teste, feature_names)\n",
    "    \n",
    "    # Gráficos de impacto global para cada modelo\n",
    "    for modelo_nome in modelos_dict.keys():\n",
    "        if resultados_shap[modelo_nome] is not None:\n",
    "            plot_impacto_global(resultados_shap, feature_names, modelo_nome)\n",
    "    \n",
    "    return resultados_shaps\n",
    "\n",
    "modelos_dict = {\n",
    "    'KNN': modelo_knn,\n",
    "    'Random Forest': modelo_rf, \n",
    "    'Gradient Boosting': modelo_gbr,\n",
    "    'Elastic Net': modelo_en,\n",
    "    'Ridge': modelo_rr\n",
    "}\n",
    "\n",
    "\n",
    "resultados, importancia_modelos = pipeline_shap_simplificada(\n",
    "    modelos_dict, X_treino, X_teste, FEATURES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e90a10-1f25-403e-8c6d-0fb05b25a996",
   "metadata": {},
   "source": [
    "---\n",
    "## **CONCLUSÃO**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
